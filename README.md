# Biases and Machine Unlearning in Language Models (LLMs)

![GitHub last commit](https://img.shields.io/github/last-commit/fabianumfalco/llm-bias-unlearning)
![Github repo stars](https://img.shields.io/github/stars/fabianumfalco/llm-bias-unlearning?color=blue&style=plastic)
![Github watchers](https://img.shields.io/github/watchers/fabianumfalco/llm-bias-unlearning?color=yellow&style=plastic)
![Github forks](https://img.shields.io/github/forks/fabianumfalco/llm-bias-unlearning?color=red&style=plastic)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Ffabianumfalco%2Fllm-bias-unlearning&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)
![Downloads](https://img.shields.io/github/downloads/fabianumfalco/llm-bias-unlearning/total?color=ccf&label=downloads&logo=github&logoColor=lightgrey)


This is curated repository of resources on bias and machine unlearning in Large Language Models. Its primary purpose is to provide a comprehensive list of relevant resources related to these topics that can support my research and dissertation, under the advisorship of [PhD Edna Dias Canedo](https://ednacanedo.github.io/), in the [Professional Graduate Program in Electrical Engineering (PPEE)](https://ppee.unb.br/) at the [University of Brasília (UnB)](https://international.unb.br/), [Department of Electrical Engineering (ENE)](http://www.ene.unb.br/).


![avatar](assets/images/DALL·E-2025-01-01-19.02.jpg "Created by DALLE")

# Table of Contents
- [Paper List :page_with_curl:](#llm-bias-unlearning)
    - [Fairness and Bias in AI](#paper-Fairness_Bias)
        - [2025](#paper-Fairness_Bias-2025)
        - [2024](#paper-Fairness_Bias-2024)
        - [2023](#paper-Fairness_Bias-2023)
        - [2022](#paper-Fairness_Bias-2022)
        - [≤ 2021](#paper-Fairness_Bias-leq-2021)
    - [Machine Unlearning](#paper-machine_unlearning)
        - [2025](#paper-machine_unlearning-2025)
        - [2024](#paper-machine_unlearning-2024)
        - [2023](#paper-machine_unlearning-2023)
        - [2022](#paper-machine_unlearning-2022) 
        - [≤ 2021](#paper-Fairness_Bias-leq-2021)   
    - [Other Related](#paper-others-related)
        - [2025](#paper-others-related-2025)
        - [2024](#paper-others-related-2024)
        - [2023](#paper-others-related-2023)
        - [2022](#paper-others-related-2022)
        - [≤ 2021](#paper-Fairness_Bias-leq-2021)
    - [Venues](#venues)      
- [Related Awesome Lists :astonished:](#related-awesome-lists)
- [Toolboxes :toolbox:](#toolboxes)
- [Seminar :alarm_clock:](#seminar) 
- [Workshops :fire:](#workshops)
- [Tutorials :woman_teacher:](#tutorials)
- [Talks :microphone:](#talks)
- [Blogs :writing_hand:](#blogs)
- [Other Resources :sparkles:](#other-resources)

# Paper List

<a id="paper-Fairness_Bias"></a>
## Fairness and Bias in AI

<a id="paper-Fairness_Bias-2025"></a>
### 2025
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|  |   |   |   |   |    |   |

<a id="paper-Fairness_Bias-2024"></a>
### 2024
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2024.08 | D. Bouchard | An actionable framework for assessing bias and fairness in large language model use cases [![Arxiv](https://img.shields.io/badge/arXiv-2407.10853-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2407.10853) | Bias, Evaluation Metrics, Fairness, Framework, Large Language Models, LLMs    | arXiv |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2407-10853) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/cvs-health/langfair) ![ ](https://img.shields.io/github/last-commit/cvs-health/langfair?style=for-the-badge)   |
|2024.04 | S. Caton and C. Haas | Fairness in machine learning: A survey [![Open](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](https://doi.org/10.1145/3616865) |  Fairness, accountability, transparency, machine learning  | ACM Comput. Surv. |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/csur/CatonH24) |  |


<a id="paper-Fairness_Bias-2023"></a>
### 2023
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2023.12 | A. F. Oketunji, M. Anas, and D. Saina | Large language model (LLM) bias index - LLMBI [![Arxiv](https://img.shields.io/badge/arXiv-2312.14769-B21A1B?style=for-the-badge)](https://doi.org/10.48550/arXiv.2312.14769) | Large Language Model, LLM, Model Calibration, Bias Quantification, Bias Mitigation, Algorithmic Fairness, Algorithmic Governance   | arXiv |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2312-14769) |  |
|2023.11 | E. Ferrara | Should ChatGPT be biased? Challenges and risks of bias in large language models [![Open](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](https://doi.org/10.5210/fm.v28i11.13346) |  Artificial Intelligence, Generative AI, Bias, Large Language Models, OpenAI, ChatGPT, GPT-3, GPT-4  | First Monday |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/firstmonday/Ferrara23a) |  |

<a id="paper-Fairness_Bias-2022"></a>
### 2022
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2022.11| N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan | A survey on bias and fairness in machine learning [![ACM](https://img.shields.io/badge/ACM-0085CA.svg?style=for-the-badge&logo=ACM&logoColor=white)](https://doi.org/10.1145/3457607) |  Fairness and Bias in Artificial Intelligence, Machine Learning, Deep Learning, Natural Language Processing, Representation Learning  | ACM Comput. Surv. |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/csur/MehrabiMSLG21) |  |

<a id="paper-Fairness_Bias-leq-2021"></a>
### ≤ 2021
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|  |   |   |   |   |    |   |

<a id="paper-machine_unlearning"></a>
## Machine Unlearning

<a id="paper-machine_unlearning-2025"></a>
### 2025
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|  |   |   |   |   |    |   |

<a id="paper-machine_unlearning-2024"></a>
### 2024
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2024.06 | J. Xu, Z. Wu, C. Wang, and X. Jia | Machine unlearning: Solutions and challenges [![IEEE](https://img.shields.io/badge/IEEE-00629B.svg?style=for-the-badge&logo=IEEE&logoColor=white)](https://doi.org/10.1109/TETCI.2024.3379240)  | Machine Unlearning; Machine Learning Security; the Right to be Forgotten  | IEEE Trans. Emerg. Top. Comput. Intell. |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/tetci/XuWWJ24) |  |
|2024.05 | A. Oesterling, J. Ma, F. P. Calmon, and H. Lakkaraju | Fair machine unlearning: Data removal while mitigating disparities [![Arxiv](https://img.shields.io/badge/arXiv-2307.14754-B21A1B?style=flat)](https://arxiv.org/abs/2307.14754) [![Open](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](https://proceedings.mlr.press/v238/oesterling24a/oesterling24a.pdf) | Data Privacy; Fair Machine Learning; Fairness; Machine Unlearning; Right to Be Forgotten  | PMLR |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/conf/aistats/OesterlingMCL24) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/AI4LIFE-GROUP/fair-unlearning) ![ ](https://img.shields.io/github/last-commit/AI4LIFE-GROUP/fair-unlearning?style=for-the-badge)   |
| 2024.05 | H. Hu, S. Wang, T. Dong, and M. Xue | Learn what you want to unlearn: Unlearning inversion attacks against machine unlearning [![IEEE](https://img.shields.io/badge/IEEE-00629B.svg?style=for-the-badge&logo=IEEE&logoColor=white)](https://doi.org/10.1109/SP54263.2024.00248) | Machine Unlearning, Privacy Vulnerability, Right to be Forgotten, Unlearning Inversion Attacks |  SP 2024 |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/conf/sp/HuWDX24) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/TASI-LAB/Unlearning-inversion-attacks) ![ ](https://img.shields.io/github/last-commit/TASI-LAB/Unlearning-inversion-attacks)  |
| 2024.05 | M. Bertrán, S. Tang, M. Kearns, J. Morgenstern, A. Roth, and Z. S. Wu | Reconstruction attacks on machine unlearning: Simple models are vulnerable [![Arxiv](https://img.shields.io/badge/arXiv-2405.20272-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2405.20272) | Data Privacy, Machine Unlearning, Privacy Risks in AI, Reconstruction Attacks  | arXiv |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2405-20272) |   |
2024.04 | Z. Liu, H. Ye, C. Chen, and K.-Y. Lam | Threats, attacks, and defenses in machine unlearning: A survey [![Arxiv](https://img.shields.io/badge/arXiv-2403.13682-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2403.13682) | Machine unlearning, threats, attacks, defenses  | arXiv |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2403-13682) |   |
|2024.03 | N. Li et al. | Machine unlearning: Taxonomy, metrics, applications, challenges, and prospects [![Arxiv](https://img.shields.io/badge/arXiv-2403.08254-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2403.08254) | Machine learning, machine unlearning, data privacy, federated learning  | arXiv |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2403-08254) |   |
| 2024.03 | J. Foster, S. Schoepf, and A. Brintrup | Fast machine unlearning without retraining through selective synaptic dampening [![Open](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](https://doi.org/10.1609/aaai.v38i11.29092) |  Machine Unlearning, Model Performance, Retrain-Free, Selective Synaptic Dampening (SSD) |  AAAI 2023 |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/conf/aaai/FosterSB24) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/AI4LIFE-GROUP/fair-unlearning) ![ ](https://img.shields.io/github/last-commit/AI4LIFE-GROUP/fair-unlearning?style=for-the-badge)   |
| 2024.02 | L. Wang, X. Zeng, J. Guo, K.-F. Wong, and G. Gottlob | Selective forgetting: Advancing machine unlearning techniques and evaluation in language models [![Arxiv](https://img.shields.io/badge/arXiv-2402.05813-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2402.05813) | Machine Unlearning, Language Model, Selective Unlearning  | arXiv |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2402-05813) |   |



<a id="paper-machine_unlearning-2023"></a>
### 2023
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2023.12 | M. Kurmanji, P. Triantafillou, J. Hayes, and E. Triantafillou | Towards unbounded machine unlearning [![Arxiv](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](http://papers.nips.cc/paper_files/paper/2023/hash/062d711fb777322e2152435459e6e9d9-Abstract-Conference.html)  | Bias Removal, Machine Unlearning, Model Utility, Right to Be Forgotten, Unlearning Algorithm | NeurIPS 2023 |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/conf/nips/KurmanjiTHT23) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/Meghdad92/SCRUB) ![ ](https://img.shields.io/github/last-commit/Meghdad92/SCRUB?style=for-the-badge)  |
|2023.08 | H. Xu, T. Zhu, L. Zhang, W. Zhou, and P. S. Yu | Machine Unlearning: A Survey [![ACN](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](https://doi.org/10.1145/3603620)  | Machine learning, deep learning, machine unlearning, sample removal, data privacy, model usabilit | ACM Comput. Surv. |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/csur/XuZZZY24) |   |

<a id="paper-machine_unlearning-2022"></a>
### 2022
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|  |   |   |   |   |    |   |

<a id="paper-machine_unlearning-leq-2021"></a>
### ≤ 2021
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2020.06 | Y. Wu, E. Dobriban, and S. B. Davidson | DeltaGrad: Rapid retraining of machine learning models [![Arxiv](https://img.shields.io/badge/arXiv-2006.14755-B21A1B?style=for-the-badge)](https://doi.org/10.48550/arXiv.2006.14755)  |  | PMLR |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.uni-trier.de/rec/conf/icml/WuDD20.html) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](hhttps://github.com/wuyinjun-1993/DeltaGrad) ![ ](https://img.shields.io/github/last-commit/wuyinjun-1993/DeltaGrad?style=for-the-badge)  |

<a id="paper-others-related"></a>
## Other Related

<a id="paper-others-related-2025"></a>
### 2025
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|  |   |   |   |   |    |   |

<a id="paper-others-related-2024"></a>
### 2024
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2024.01 | B. C. Das, M. H. Amini, and Y. Wu | Security and privacy challenges of large language models: A survey [![Arxiv](https://img.shields.io/badge/arXiv-2402.00888-B21A1B?style=for-the-badge)](https://doi.org/10.48550/arXiv.2402.00888) | Large Language Models, Security and Privacy Challenges, Defense Mechanisms.   | arXiv |  [![dblp](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2402-00888) |  |

<a id="paper-others-related-2023"></a>
### 2023
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2023.12 | Y. Yao, J. Duan, K. Xu, Y. Cai, E. Sun, and Y. Zhang | A survey on large language model (LLM) security and privacy: The good, the bad, and the ugly [![Arxiv](https://img.shields.io/badge/arXiv-2312.02003-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2312.02003)  | Large Language Model (LLM), LLM Security, LLM Privacy, ChatGPT, LLM Attacks, LLM Vulnerabilities  | arXiv |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2312-02003) |  |

<a id="paper-others-related-2022"></a>
### 2022
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|  |   |   |   |   |    |   |

<a id="paper-others-related-leq-2021"></a>
### ≤ 2021
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|  |   |   |   |   |    |   |

## Venues

In the context of a research paper, "venue" refers to the specific conference, journal, symposium, or workshop where the paper was published or presented. 

| Acronym | Venue | 
|:--------- | ----- | 
|  AAAI  | Association for the Advancement of Artificial IntelligenceSponsorship |
| ACM Comput. Surv. | ACM Computing Surveys |
| IEEE Trans. Emerg. Top. Comput. Intell. | IEEE Transactions on Emerging Topics in Computational Intelligence |
| NeurIPS | Annual Conference on Neural Information Processing Systems |
| PMLR | Proceedings of Machine Learning Research |
|  SP | IEEE symposium on security and privacy|


# Related Awesome Lists

| **Title** | **User GitHub** | **Topics** |  | 
| --------------- | ---- | ---- | ---- | 
| [Awesome Attacks on Machine Learning Privacy](https://github.com/stratosphereips/awesome-ml-privacy-attacks) | [stratosphereips](https://github.com/stratosphereips) |[Machine-Unlearning](https://github.com/topics/machine-unlearning)  [Privacy](https://github.com/topics/privacy) | ![ ](https://img.shields.io/github/last-commit/stratosphereips/awesome-ml-privacy-attacks?style=for-the-badge) ![ ](https://img.shields.io/github/stars/stratosphereips/awesome-ml-privacy-attacks) |
| [Awesome Bias and Fairness Datasets and Benchmarks in Language Models](https://github.com/richhh520/Awesome_Bias_and_Fairness_Datasets_and_Benchmarks) | [richhh520](https://github.com/richhh520) |[Bias-AI](https://github.com/topics/Bias-AI) [Fairness-AI](https://github.com/topics/Fairness-AI) | ![ ](https://img.shields.io/github/last-commit/richhh520/Awesome_Bias_and_Fairness_Datasets_and_Benchmarks?style=for-the-badge) ![ ](https://img.shields.io/github/stars/richhh520/Awesome_Bias_and_Fairness_Datasets_and_Benchmarks) |
| [Awesome-GenAI-Unlearning](https://github.com/franciscoliu/Awesome-GenAI-Unlearning) | [franciscoliu](https://github.com/franciscoliu) |[Generative-AI](https://github.com/topics/Generative-AI) [Machine-Unlearning](https://github.com/topics/Machine-Unlearning) | ![ ](https://img.shields.io/github/last-commit/franciscoliu/Awesome-GenAI-Unlearning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/franciscoliu/Awesome-GenAI-Unlearning) |
| [Awesome Large Language Model Unlearning](https://github.com/chrisliu298/awesome-llm-unlearning) | [chrisliu298](https://github.com/chrisliu298)| [Machine-Unlearning](https://github.com/topics/machine-unlearning) [LLM-Unlearning](https://github.com/topics/llm-unlearning) | ![ ](https://img.shields.io/github/last-commit/chrisliu298/awesome-llm-unlearning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/chrisliu298/awesome-llm-unlearning) |
| [Awesome Machine Unlearning](https://github.com/tamlhp/awesome-machine-unlearning) | [tamlhp](https://github.com/tamlhp) |[Machine-Unlearning](https://github.com/topics/machine-unlearning)  | ![ ](https://img.shields.io/github/last-commit/tamlhp/awesome-machine-unlearning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/tamlhp/awesome-machine-unlearning) | |
| [Awesome Trustworthy Deep Learning](https://github.com/MinghuiChen43/awesome-trustworthy-deep-learning) | [MinghuiChen43](https://github.com/MinghuiChen43) |[Trustworthy-AI](https://github.com/topics/trustworthy-ai) | ![ ](https://img.shields.io/github/last-commit/MinghuiChen43/awesome-trustworthy-deep-learning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/MinghuiChen43/awesome-trustworthy-deep-learning) |
| [LLM-Unlearning-Paper-List](https://github.com/KID-22/LLM-Unlearning-Paper-List) | [KID-22](https://github.com/KID-22) |[Machine-Unlearning](https://github.com/topics/machine-unlearning) [LLM-Unlearning](https://github.com/topics/llm-unlearning) | ![ ](https://img.shields.io/github/last-commit/KID-22/LLM-Unlearning-Paper-List?style=for-the-badge) ![ ](https://img.shields.io/github/stars/KID-22/LLM-Unlearning-Paper-List) |
| [Machine Unlearning Papers](https://github.com/jjbrophy47/machine_unlearning) | [jjbrophy47](https://github.com/jjbrophy47) |[Machine-Unlearning](https://github.com/topics/machine-unlearning)  | ![ ](https://img.shields.io/github/last-commit/jjbrophy47/machine_unlearning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/jjbrophy47/machine_unlearning) |


# Toolboxes

# Seminar

# Workshops

# Talks

# Blogs

# Tutorials

# Other Resources