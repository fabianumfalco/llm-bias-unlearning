# Biases and Machine Unlearning in Language Models (LLMs)

![GitHub last commit](https://img.shields.io/github/last-commit/fabianumfalco/llm-bias-unlearning)
![Github repo stars](https://img.shields.io/github/stars/fabianumfalco/llm-bias-unlearning?color=blue&style=plastic)
![Github watchers](https://img.shields.io/github/watchers/fabianumfalco/llm-bias-unlearning?color=yellow&style=plastic)
![Github forks](https://img.shields.io/github/forks/fabianumfalco/llm-bias-unlearning?color=red&style=plastic)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Ffabianumfalco%2Fllm-bias-unlearning&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)


This is curated repository of resources on bias and machine unlearning in Large Language Models. Its primary purpose is to provide a comprehensive list of relevant resources related to these topics that can support my research and dissertation, under the advisorship of [PhD Edna Dias Canedo](https://ednacanedo.github.io/), in the [Professional Graduate Program in Electrical Engineering (PPEE)](https://ppee.unb.br/) at the [University of Brasília (UnB)](https://international.unb.br/), [Department of Electrical Engineering (ENE)](http://www.ene.unb.br/).


![avatar](assets/images/DALL·E-2025-01-01-19.02.jpg "Created by DALLE")

# Table of Contents
- [Paper List :page_with_curl:](#llm-bias-unlearning)
    - [Fairness and Bias in AI](#paper-Fairness_Bias)
        - [2024](#paper-Fairness_Bias-2024)
        - [2023](#paper-Fairness_Bias-2023)
        - [2022](#paper-Fairness_Bias-2022)
    - [Machine Unlearning](#paper-machine_unlearning)
        - [2024](#paper-machine_unlearning-2024)
        - [2023](#paper-machine_unlearning-2023)
        - [2022](#paper-machine_unlearning-2022)    
    - [Other Related](#paper-others-related)
        - [2024](#paper-others-related-2024)
        - [2023](#paper-others-related-2023)
        - [2022](#paper-others-related-2022)
    - [Venues](#venues)      
- [Related Awesome Lists :astonished:](#related-awesome-lists)
- [Toolboxes :toolbox:](#toolboxes)
- [Seminar :alarm_clock:](#seminar) 
- [Workshops :fire:](#workshops)
- [Tutorials :woman_teacher:](#tutorials)
- [Talks :microphone:](#talks)
- [Blogs :writing_hand:](#blogs)
- [Other Resources :sparkles:](#other-resources)

# Paper List

<a id="paper-Fairness_Bias"></a>
## Fairness and Bias in AI

<a id="paper-Fairness_Bias-2024"></a>
### 2024
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2024.08 | D. Bouchard | An actionable framework for assessing bias and fairness in large language model use cases [![Arxiv](https://img.shields.io/badge/arXiv-2407.10853-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2407.10853) | Bias, Evaluation Metrics, Fairness, Framework, Large Language Models, LLMs    | arXiv |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2407-10853) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/cvs-health/langfair) ![ ](https://img.shields.io/github/last-commit/cvs-health/langfair?style=for-the-badge)   |
|2024.04 | S. Caton and C. Haas | Fairness in machine learning: A survey [![Open](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](https://doi.org/10.1145/3616865) |  Fairness, accountability, transparency, machine learning  | ACM Comput. Surv. |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/csur/CatonH24) |  |


<a id="paper-Fairness_Bias-2023"></a>
### 2023
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2023.12 | A. F. Oketunji, M. Anas, and D. Saina | Large language model (LLM) bias index - LLMBI [![Arxiv](https://img.shields.io/badge/arXiv-2312.14769-B21A1B?style=for-the-badge)](https://doi.org/10.48550/arXiv.2312.14769) | Large Language Model, LLM, Model Calibration, Bias Quantification, Bias Mitigation, Algorithmic Fairness, Algorithmic Governance   | arXiv |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2312-14769) |  |

<a id="paper-Fairness_Bias-2022"></a>
### 2022
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2022.11| N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan | A survey on bias and fairness in machine learning [![ACM](https://img.shields.io/badge/ACM-0085CA.svg?style=for-the-badge&logo=ACM&logoColor=white)](https://doi.org/10.1145/3457607) |  Fairness and Bias in Artificial Intelligence, Machine Learning, Deep Learning, Natural Language Processing, Representation Learning  | ACM Comput. Surv. |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/csur/MehrabiMSLG21) |  |

<a id="paper-machine_unlearning"></a>
## Machine Unlearning

<a id="paper-machine_unlearning-2024"></a>
### 2024
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2024.06 | J. Xu, Z. Wu, C. Wang, and X. Jia | Machine unlearning: Solutions and challenges [![Arxiv](https://img.shields.io/badge/IEEE-00629B.svg?style=for-the-badge&logo=IEEE&logoColor=white)](https://doi.org/10.1109/TETCI.2024.3379240)  | Machine Unlearning; Machine Learning Security; the Right to be Forgotten  | IEEE Trans. Emerg. Top. Comput. Intell. |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/tetci/XuWWJ24) |  |
|2024.05 | A. Oesterling, J. Ma, F. P. Calmon, and H. Lakkaraju | Fair machine unlearning: Data removal while mitigating disparities [![Arxiv](https://img.shields.io/badge/arXiv-2307.14754-B21A1B?style=flat)](https://arxiv.org/abs/2307.14754) [![Open](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](https://proceedings.mlr.press/v238/oesterling24a/oesterling24a.pdf) | Data Privacy; Fair Machine Learning; Fairness; Machine Unlearning; Right to Be Forgotten  | PMLR |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/conf/aistats/OesterlingMCL24) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/AI4LIFE-GROUP/fair-unlearning) ![ ](https://img.shields.io/github/last-commit/AI4LIFE-GROUP/fair-unlearning?style=for-the-badge)   |
|2024.03 | N. Li et al. | Machine unlearning: Taxonomy, metrics, applications, challenges, and prospects [![Arxiv](https://img.shields.io/badge/arXiv-2403.08254-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2403.08254) | Machine learning, machine unlearning, data privacy, federated learning  | arXiv |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2403-08254) |   |
2024.02 | L. Wang, X. Zeng, J. Guo, K.-F. Wong, and G. Gottlob | Selective forgetting: Advancing machine unlearning techniques and evaluation in language models [![Arxiv](https://img.shields.io/badge/arXiv-2402.05813-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2402.05813) | Machine Unlearning, Language Model, Selective Unlearning  | arXiv |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](hhttps://dblp.org/rec/journals/corr/abs-2402-05813) |   |
2024.04 | Z. Liu, H. Ye, C. Chen, and K.-Y. Lam | Threats, attacks, and defenses in machine unlearning: A survey [![Arxiv](https://img.shields.io/badge/arXiv-2403.13682-B21A1B?style=flat)](https://doi.org/10.48550/arXiv.2403.13682) |   | arXiv |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/journals/corr/abs-2403-13682) |   |


<a id="paper-machine_unlearning-2023"></a>
### 2023
| Date | Author(s) | Title | Keywords | Venue |  Bib Source | Code |
|:--------- | ----- | ----- | ----- | ----- |  ----- | ----- |
|2023.12 | M. Kurmanji, P. Triantafillou, J. Hayes, and E. Triantafillou | Towards unbounded machine unlearning [![Arxiv](https://img.shields.io/badge/Open%20Access-F68212.svg?style=flat&logo=Open-Access&logoColor=white)](http://papers.nips.cc/paper_files/paper/2023/hash/062d711fb777322e2152435459e6e9d9-Abstract-Conference.html)  | Bias Removal, Machine Unlearning, Model Utility, Right to Be Forgotten, Unlearning Algorithm, | NeurIPS |  [![GitHub](https://img.shields.io/badge/dblp-004F9F.svg?style=for-the-badge&logo=dblp&logoColor=white)](https://dblp.org/rec/conf/nips/KurmanjiTHT23) | [![GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/meghdadk/SCRUB) ![ ](https://img.shields.io/github/last-commit/meghdadk/SCRUB?style=for-the-badge)  |

<a id="paper-machine_unlearning-2022"></a>
### 2022
...

<a id="paper-others-related"></a>
## Other Related

<a id="paper-others-related-2024"></a>
### 2024
...

<a id="paper-others-related-2023"></a>
### 2023
...

<a id="paper-others-related-2022"></a>
### 2022
...

## Venues

In the context of a research paper, "venue" refers to the specific conference, journal, symposium, or workshop where the paper was published or presented. 

| Acronym | Venue | 
|:--------- | ----- | 
| ACM Comput. Surv. | ACM Computing Surveys |
| IEEE Trans. Emerg. Top. Comput. Intell. | IEEE Transactions on Emerging Topics in Computational Intelligence |
| NeurIPS | Annual Conference on Neural Information Processing Systems |
| PMLR | Proceedings of Machine Learning Research |


# Related Awesome Lists

| **Title** | **User GitHub** | **Topics** |  | 
| --------------- | ---- | ---- | ---- | 
| [Awesome Attacks on Machine Learning Privacy](https://github.com/stratosphereips/awesome-ml-privacy-attacks) | [stratosphereips](https://github.com/stratosphereips) |[Machine-Unlearning](https://github.com/topics/machine-unlearning)  [Privacy](https://github.com/topics/privacy) | ![ ](https://img.shields.io/github/last-commit/stratosphereips/awesome-ml-privacy-attacks?style=for-the-badge) ![ ](https://img.shields.io/github/stars/stratosphereips/awesome-ml-privacy-attacks) |
| [Awesome Bias and Fairness Datasets and Benchmarks in Language Models](https://github.com/richhh520/Awesome_Bias_and_Fairness_Datasets_and_Benchmarks) | [richhh520](https://github.com/richhh520) |[Bias-AI](https://github.com/topics/Bias-AI) [Fairness-AI](https://github.com/topics/Fairness-AI) | ![ ](https://img.shields.io/github/last-commit/richhh520/Awesome_Bias_and_Fairness_Datasets_and_Benchmarks?style=for-the-badge) ![ ](https://img.shields.io/github/stars/richhh520/Awesome_Bias_and_Fairness_Datasets_and_Benchmarks) |
| [Awesome-GenAI-Unlearning](https://github.com/franciscoliu/Awesome-GenAI-Unlearning) | [franciscoliu](https://github.com/franciscoliu) |[Generative-AI](https://github.com/topics/Generative-AI) [Machine-Unlearning](https://github.com/topics/Machine-Unlearning) | ![ ](https://img.shields.io/github/last-commit/franciscoliu/Awesome-GenAI-Unlearning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/franciscoliu/Awesome-GenAI-Unlearning) |
| [Awesome Large Language Model Unlearning](https://github.com/chrisliu298/awesome-llm-unlearning) | [chrisliu298](https://github.com/chrisliu298)| [Machine-Unlearning](https://github.com/topics/machine-unlearning) [LLM-Unlearning](https://github.com/topics/llm-unlearning) | ![ ](https://img.shields.io/github/last-commit/chrisliu298/awesome-llm-unlearning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/chrisliu298/awesome-llm-unlearning) |
| [Awesome Machine Unlearning](https://github.com/tamlhp/awesome-machine-unlearning) | [tamlhp](https://github.com/tamlhp) |[Machine-Unlearning](https://github.com/topics/machine-unlearning)  | ![ ](https://img.shields.io/github/last-commit/tamlhp/awesome-machine-unlearning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/tamlhp/awesome-machine-unlearning) | |
| [Awesome Trustworthy Deep Learning](https://github.com/MinghuiChen43/awesome-trustworthy-deep-learning) | [MinghuiChen43](https://github.com/MinghuiChen43) |[Trustworthy-AI](https://github.com/topics/trustworthy-ai) | ![ ](https://img.shields.io/github/last-commit/MinghuiChen43/awesome-trustworthy-deep-learning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/MinghuiChen43/awesome-trustworthy-deep-learning) |
| [LLM-Unlearning-Paper-List](https://github.com/KID-22/LLM-Unlearning-Paper-List) | [KID-22](https://github.com/KID-22) |[Machine-Unlearning](https://github.com/topics/machine-unlearning) [LLM-Unlearning](https://github.com/topics/llm-unlearning) | ![ ](https://img.shields.io/github/last-commit/KID-22/LLM-Unlearning-Paper-List?style=for-the-badge) ![ ](https://img.shields.io/github/stars/KID-22/LLM-Unlearning-Paper-List) |
| [Machine Unlearning Papers](https://github.com/jjbrophy47/machine_unlearning) | [jjbrophy47](https://github.com/jjbrophy47) |[Machine-Unlearning](https://github.com/topics/machine-unlearning)  | ![ ](https://img.shields.io/github/last-commit/jjbrophy47/machine_unlearning?style=for-the-badge) ![ ](https://img.shields.io/github/stars/jjbrophy47/machine_unlearning) |


# Toolboxes

# Seminar

# Workshops

# Talks

# Blogs

# Tutorials

# Other Resources